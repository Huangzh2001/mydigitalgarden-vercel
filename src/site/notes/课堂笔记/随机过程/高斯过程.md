---
{"tags":["Notes","Math","StochasticProcess"],"version":1,"dg-publish":true,"permalink":"/课堂笔记/随机过程/高斯过程/","dgPassFrontmatter":true}
---

# 高斯分布无处不在(Gaussian Everywhere)

## 布朗运动与爱因斯坦扩散(Diffusion)方程

我们在一个装满水的管子中滴入一滴墨水，我们想知道在经过时间$t$后，墨水在$x$位置的分布情况$f(x,t)$。

![](/img/user/课堂笔记/随机过程/attachments/高斯过程.png)

为了计算出$f(x,t)$，爱因斯坦引入了$\rho(y,\tau)$：单个粒子在$\tau$时间内能跑出$y$距离的概率。并假设：
$$
\begin{cases}\int_{-\infty}^{+\infty}p(y,\tau)dy=1\\\\\int_{-\infty}^{+\infty}y\rho(y,\tau)dy=0\quad\bigg(\rho(y,\tau)=\rho(-y,\tau)\bigg)\\\\\int_{-\infty}^{+\infty}y^2\rho(y,\tau)dy=D(\tau)\end{cases}
$$
（其中第二个假设是因为爱因斯坦认为粒子往左跑和往右跑的数量应该是对称的）

然后我们就得到了下面的主方程：
$$
f(x,t+\tau)=\int_{-\infty}^{+\infty}f(x-y,t)\rho(y,\tau)dy
$$

我们对上述方程的$f(x-y,t)$进行泰勒展开。（按照我们目前的理论，这里是不能进行泰勒展开的。但是爱因斯坦的结果跟实验结果高度地一致，所以我们应该怀疑自己的理论错了而不是爱因斯坦错了）

$$
\begin{aligned}f(x,t+\tau)&=\int_{-\infty}^{+\infty}f(x-y,t)\rho(y,\tau)dy\\&=\int_{-\infty}^{+\infty}\left(f(x,t)+(-y)\frac{\partial}{\partial x}f(x,t)+\frac{y^2}{2}\frac{\partial^2}{\partial x^2}f(x,t)+\cdots\right)\rho(y,\tau)dy\\&=f(x,t)+\frac{D(\tau)}{2}\frac{\partial^2}{\partial x^2}f(x,t)\end{aligned}
$$
（其中最后一步我们把高阶项$o(y^2)$忽略掉了）即
$$
f(x,t+\tau)-f(x,t)=\frac{D(\tau)}{2}\frac{\partial^{2}}{\partial x^{2}}f(x,t)
$$

我们两边除以$\tau$就得：
$$
\frac{f(x,t+\tau)-f(x,t)}{\tau}=\frac{1}{2}\frac{D(\tau)}{\tau}\frac{\partial^{2}}{\partial x^{2}}f(x,t)
$$

由于当$\tau$趋于0的时候（相当于时间无穷小），$\rho(y,\tau)$（即单个粒子在$\tau$时间内能跑出$y$距离的概率）的方差$D(\tau)$应该趋于0（因为时间无穷小，粒子应该大概率留在原地）。于是我们假设$\frac{D(\tau)}{\tau}\xrightarrow{\tau\rightarrow0}D$，于是对两边取极限$\tau\to0$可得
$$
\frac{\partial f}{\partial t}=\frac{D}{2}\frac{\partial^{2}f}{\partial x^{2}}
$$

在给定初值条件：$f(x,0)=\delta(x)$（物理意义是在0时刻在0点有墨水，其他地方没有墨水），解这个偏微分方程（学过偏微分方程的应该能看出这就是扩散方程）就可得：
$$
f(x,t)=\frac{1}{\sqrt{2\pi Dt}}\exp(-\frac{x^{2}}{2Dt})
$$

即墨水粒子的分布服从高斯分布。
## 随机游动(Random Walk)

假设粒子在一维空间中随机游动,每一段时间内只能走一格,且往左走还是往右走的概率相等.设每走一格在空间上是$\Delta x$，在时间上是$\Delta t$。

设$P(m,n)$为粒子在n时刻位于m点的概率,则随机游走的主方程为：
$$
P(m,n)=\frac12(P(m+1,n-1)+P(m-1,n-1))
$$

下面我们来证明：在附加一定的条件下，当$\Delta x\to 0,\Delta t\to 0$时，$P$服从高斯分布。
$$
P(m,n+1)-P(m,n)=\frac{1}{2}(P(m+1,n)+P(m-1,n)-2P(m,n))
$$

（上式的左边是时间上的差分；右边是空间上的二阶差分$(P(m+1,n)-P(m,n))-(P(m,n)-P(m-1,n))$）

$$
\frac{P\left(m,n+1\right)-P\left(m,n\right)}{\Delta t}=\frac12\frac{P\left(m+1,n\right)+P\left(m-1,n\right)-2P\left(m,n\right)}{\left(\Delta{x}\right)^{2}}\frac{\left(\Delta x\right)^{2}}{\Delta t}
$$

我们假设$\frac{\left(\Delta x\right)^{2}}{\Delta t}$是个常数，不妨设为$\frac{\left(\Delta x\right)^{2}}{\Delta t}=D$。于是令$\Delta t\to0,\Delta x\to0$有

$$
\frac{\partial P}{\partial t}=\frac{D}{2}\frac{\partial^{2}p}{\partial x^{2}}
$$

这不就是前面的扩散方程吗？所以一定服从高斯分布。

```ad-info
我们可以用中心极限定理再证明一遍：

设$S_n$为经历了n个$\Delta t$后的位置，则$S_n=X_1+\cdots+X_n$，其中$X_k\sim \left.\left(\begin{matrix}1&-1\\\frac{1}{2}&\frac{1}{2}\end{matrix}\right.\right)$

设$B_n$为经历了n个$\Delta t$后向右走的次数，则
$$
\begin{aligned}S_n&=B_n\Delta X+(n-B_n)(-\Delta X)\\\\&=(2B_n-n)\Delta X\\\\&=2(B_n-\frac{n}{2})\Delta X\end{aligned}
$$

则$B_{n}=Y_{1}+\cdots+Y_{n}$，其中$Y_k\sim \left.\left(\begin{matrix}1&0\\\frac{1}{2}&\frac{1}{2}\end{matrix}\right.\right)$

于是$Y_n$的均值和方差为$E(Y_{n})=\frac{1}{2},var(B_n)=\frac{1}{4}$

于是
$$
2(B_{n}-\frac{n}{2})=\sum_{k=1}^{n}\frac{(Y_{k}-\frac{1}{2})}{\frac{1}{2}}
$$
是n个均值为0，方差为1的独立同分布的随机变量之和。所以可以使用中心极限定理，即：
$$
\begin{aligned}
2\left(B_{n}-\frac{n}{2}\right)\Delta x&=\frac{B_{n}-\frac{n}{2}}{\frac{\sqrt{n}}{2}}\sqrt{n}\Delta x \\
&\xrightarrow{n\rightarrow\infty}N(0,1)\sqrt{\frac{t}{\Delta t}}\Delta x \\
&=N(0,1)\sqrt{\frac{t(\Delta x)^{2}}{\Delta t}} \\
&=N(0,\frac{t(\Delta x)^{2}}{\Delta t}) \\
&=N\left(0,Dt\right)
\end{aligned}
$$
```

## 中心极限定理

我们前面学过的中心极限定理也是跟高斯分布紧密相关.

```ad-theorem[中心极限定理]
设$X_1,\cdots,X_n$是随机变量，且$E(X_k)=0$，$var(X_k)=1$，则
$$
\frac{1}{\sqrt{n}}\sum_{k=1}^{n}X_{k}\xrightarrow{F}N(0,1)
$$
```

## 最大熵(Maximum Entropy)

设$f(x)$是概率密度函数，设$H(f)=-\int_{-\infty}^{+\infty}f(x)\log f(x)dx$(称为"熵")，我们希望在固定其均值和方差的情况下求$f$使得$H(f)$最大。即固定$\begin{cases}\int_{-\infty}^{+\infty}xf(x)dx=\mu\\\int_{-\infty}^{+\infty}\left(x-\mu\right)^{2}f(x)dx=\sigma^{2}\end{cases}$，求$\underset{f}{\max}H(f)$（称为"最大熵"）

我们在这里采用变分原理(Variational Principle)：

假设$f_0$是最优解，则$H(f)\leq H(f_0),\forall f$。

我们构造$h(t)=H(f_{0}+tg)=-\int_{-\infty}^{+\infty}(f_{0}+tg)\log(f_{0}+tg)dx$，则$h(t)\leq h(0)$，即0是最优点。于是我们构造拉格朗日函数：
$$
\begin{aligned}
L(t,\lambda_0,\lambda_1,\lambda_2)=&-\int_{-\infty}^{+\infty}(f_0+tg)\log{(f_0+tg)}dx\\
&+\lambda_{0}\left(\int_{-\infty}^{+\infty}(f_{0}+tg)dx-1\right)\\
&+\lambda_1\left(\int_{-\infty}^{+\infty}x(f_0+tg)dx-\mu\right)\\
&+\lambda_2\left(\int_{-\infty}^{+\infty}\left(x-\mu\right)^2\left(f_0+tg\right)dx-\sigma^2\right)
\end{aligned}
$$

于是有：
$$
0=\frac{d}{dt}~L|_{t=0}~=~\int_{-\infty}^{+\infty}(-g\log(f_{0})-g+\lambda_{0}g+\lambda_{1}x g+\lambda_{2}(x-\mu)^{2}g)dx
$$
即
$$
0=\int_{-\infty}^{+\infty}\left(-\log\left(f_{0}(x)\right)-1+\lambda_{0}+\lambda_{1}x+\lambda_{2}(x-u)^{2}\right)g(x)dx
$$
对任意的$g$都成立，于是有
$$
\begin{array}{ll}{}&{{-\log\left(f_{0}(x)\right)-1+\lambda_{0}+\lambda_{1}x+\lambda_{2}\left(x-\mu\right)^{2}=0}}\\\\&{\Longrightarrow}{{\log\left(f_{0}(x)\right)=\lambda_{2}\left(x-\mu\right)^{2}+\lambda_{1}x+\lambda_{0}-1}}\\\\&{\Longrightarrow}{{f_{0}(x)=\exp\left(\lambda_{2}(x-\mu)^{2}+\lambda_{1}x+\lambda_{0}-1\right)}}\\\end{array}
$$

所以$f_0(x)$一定是高斯分布。

```ad-info
熵可以理解为随机的程度。从上述讨论可以看出，当给定均值，给定方差时，最大熵分布一定是高斯分布。这说明当给定均值，给定方差时，高斯分布最随机。
```

# 高斯分布ABC

由于高斯过程跟高斯分布及多元高斯分布密切相关，所以在学习高斯过程之前，我们很有必要复习一下有关高斯分布的基础知识。
## n维高斯分布的概率密度函数

设$X\sim N(\mu,\Sigma)$，则
$$
f_{X}(x)=\frac1{(2\pi)^{\frac n2}(\det\Sigma)^{\frac12}}\exp{(-\frac12(x-\mu)^{T}\Sigma^{-1}(x-\mu))},~~~x=(x_1,\cdots,x_n)\in \mathbb R^n
$$
其中$\mu=E(X),\Sigma=E(X-\mu)\left(X-\mu\right)^\mathrm{T}$

## n维高斯分布的性质

$$
\int_{\mathbb R^{n}}f_{X}(x)dx=1
$$

```ad-proof
由于$\Sigma$正定对称，于是存在正交矩阵$U$，使得$\Sigma=U^{T}\Lambda U$，其中$\Lambda$是对角元为正的对角矩阵，于是可以写成$\Lambda=\Lambda^{\frac{1}{2}}\Lambda^{\frac{1}{2}}$

于是有
$$
\begin{aligned}(x-\mu)^{T}\Sigma^{-1}(x-\mu)&=(x-\mu)^{T}U^{T}\Lambda^{-1}U(x-\mu)\\&=(x-\mu)^{T}U^{T}\Lambda^{-\frac{1}{2}}\Lambda^{-\frac{1}{2}}U(x-\mu)\\&=Y^{T}Y\end{aligned}
$$
其中$Y=\Lambda^{-\frac{1}{2}}U(x-\mu)$，我们计算这个变换的雅可比矩阵为
$$
\begin{aligned}d Y&=|\det J|\cdot dx\\&=|\det(\Lambda^{-\frac12}\cdot U)|dx\\&=|\det \Lambda^{-\frac12}\cdot\det U|dx\\&=|\det \Lambda^{-\frac12}|dx\\&=(\det\Sigma)^{-\frac12}dx\end{aligned}
$$
于是有：
$$
\begin{aligned}
&\int_{\mathbb R^{n}}\frac{1}{(2\pi)^{\frac n2}(det\Sigma)^{\frac{1}{2}}}\exp\left(-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)\right)dx\\\\
&=\int_{\mathbb{R}^n}\frac{1}{(2\pi)^{\frac n2}(\mathrm{det}\Sigma)^{\frac{1}{2}}}\exp\left(-\frac{1}{2}Y^{T}Y\right)\left(\mathrm{det}\Sigma\right)^{\frac{1}{2}}dY\\\\&=\frac{1}{(2\pi)^{\frac n2}}\int_{-\infty}^{+\infty}\cdots\int_{-\infty}^{+\infty}\exp\left(-\frac{1}{2}\sum_{k=1}^{n}y_{k^{2}}\right)dy_{1}\cdots dy_{n}\\\\&=\left(\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}\exp\left(-\frac{y^{2}}{2}\right)dy\right)^{n}=1
\end{aligned}
$$
```

## n维高斯分布的特征函数

```ad-info
随机向量的特征函数：

设$X=(X_1,\cdots,X_n)^T\sim f_X(x),x=(x_1\cdots,x_n)\in \mathbb R^n$，则特征函数的定义为：
$$
\begin{aligned}\phi_{X}(w)&=E(\exp(jw^Tx))=E(\exp(j(w_1x_1+\cdots+w_nx_n)))\\&=\int_{\mathbb{R}^n}\exp(jw^Tx)f_{X}(x)dx\end{aligned}
$$

其中$j$满足$j^2=-1$.
```

$$
\phi_{X}(w)=\frac{1}{(2\pi)^{\frac{n}{2}}(\det\Sigma)^{\frac{n}{2}}}\int_{\mathbb R^{n}}\exp\left(jw^{T}x\right)\exp\left(-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)\right)dx
$$
我们这里用一个小技巧：先将n为化为1维：
$$
jw^{T}x-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)\overset{\text{1维}}{\rightarrow}jwx-\frac{1}{2}\frac{1}{\sigma^{2}}(x-\mu)^{2}
$$
化简后再将1维化回n维：
$$
\begin{aligned}
&jwx-\frac{1}{2}\frac{1}{\sigma^{2}}(x-\mu)^{2}\\
&\overset{\text{化简}}{=}-\frac{1}{2\sigma^{2}}\left(x-\mu-j\sigma^{2}w\right)^{2}+j\mu w-\frac{1}{2}\sigma^{2}w^{2}\\
&\xrightarrow{\text{翻译}}  { - \frac 1 2 }\left(x-\mu-j\Sigma w\right)^{T}\Sigma^{-1}\left(x-\mu-j\Sigma\omega\right)+j\mu^{T}\omega-\frac12\omega^{T}\Sigma\omega 
\end{aligned}
$$
所以
$$
\begin{aligned}\phi_{X}\left(w\right)&=\frac1{\left(2\pi\right)^{\frac n2}\left(\det\Sigma\right)^{\frac12}}\int_{\mathbb{R}^n}\exp\left(-\frac12\left(x-\mu-j\Sigma w\right)^{T}\Sigma^{-1}\left(x-\mu-j\Sigma w\right)\right)dx\exp\left(j\mu^{T}w-\frac12w^{T}\Sigma w\right)\\&=\exp\left(j\mu^{T}w-\frac12w^{T}\Sigma w\right)\end{aligned}
$$

```ad-remark
当$n=1,\mu=0,\sigma^2=1$时，我们可以得到一维高斯的特征函数：$\exp(-\frac{\omega^2}{2})$。
```

## 高斯分布经过线性变换后还是高斯分布

```ad-proposition
设$X\sim N(\mu,\Sigma)\in \mathbb R^n$是n维高斯分布，$A\in \mathbb R^{m\times n}$，$Y=AX$，则$Y\sim N(A\mu,A\Sigma A^T)$
```

```ad-proof
$$
\begin{aligned}\phi_Y(w)&=\mathbb{E}\left(\exp(jw^TY)\right)\\&=\mathbb{E}\left(\exp(jw^TAX)\right)\\&=E\left(\exp\left(j(A^Tw)^Tx)\right)\right)\\&=\phi_{X}\left(A^Tw\right)\\&=\exp\left(j(A\mu)^Tw-\frac12w^T(A\Sigma A^T)w\right)\end{aligned}
$$
我们很容易看出最后一个式子是高斯分布$N(A\mu,A\Sigma A^T)$的特征函数的形式.
```

## 联合高斯一定有边缘高斯

```ad-proposition
$X=(X_1,\cdots,X_n)^T\sim N\Longrightarrow (X_{n_1},X_{n_2},\cdots,X_{n_k})^T\sim N$，其中$\{n_1,\cdots,n_k\}\subseteq \{1,\cdots,n\}$
```

```ad-proof
我们只要做一个这样子的线性变换：
$$
\begin{pmatrix}X_{n_1}\\\vdots\\X_{n_2}\\\vdots\\X_{n_2}\end{pmatrix}=\begin{pmatrix}0&\cdots0&1&0&\cdots&0\\0&\cdots1&0&0&\cdots&0\\\vdots\\0&\cdots0&0&1&\cdots&0\end{pmatrix}\begin{pmatrix}X_1\\\vdots\\\vdots\\X_n\end{pmatrix}
$$
其中第k行只有第$n_k$个元素为1，其他都为0。
```

```ad-remark
这个命题的逆命题不成立（很容易就能构造一个反例），即所有$X_k$都是高斯分布，它们的联合分布却不一定是高斯分布。但如果我们加上一个条件：它们的任意线性组合也是高斯分布，那么它们的联合分布一定是高斯分布。这就是下面一个命题。
```

```ad-proposition
$X\in\mathbb R^n,X\sim N\Longleftrightarrow \forall\alpha\in \mathbb R^n,\alpha^T X\sim N$
```

```ad-proof

(1) 左推右可以由高斯分布经过线性变换后还是高斯分布得出

(2) 右推左

我们发现
$$
\phi_{X}(w)=E\left(\exp(jw^{T}X)\right)=\phi_{w^{T}X}(1)=\exp\left(j\mu_{w^{T}X}\cdot 1-\frac{1}{2}\sigma_{w^{T}X}^{2}\cdot1^{2}\right)
$$
而我们算出$\omega^T X$的均值和方差为
$$
\begin{aligned}
&\mu_{w^{T}X}=E\left(w^{T}X\right)=w^{T}E\left(X\right)=w^{T}\mu\\
&\begin{aligned}
\sigma^{2}_{w^{T}X}=E\left(w^{T}X-E\left(w^{T}X\right)\right)^{2}&=E\left(w^{T}\left(X-\mu\right)\right)^{2}\\
&=w^{T}E(X-\mu)(X-\mu)^{T}w\\&=w^{T}\Sigma_{X}w
\end{aligned}
\end{aligned}
$$
代入得
$$
\phi_{X}(w)=\exp(jw^T\mu-\frac12w^T\Sigma_{X}w)
$$
我们很容易看出这是高斯分布特征函数的形式。

```

## 在高斯分布中，不相关等价于独立

```ad-info
我们知道，在一般情况下，独立能推出不相关，但不相关不能推出独立。
```

```ad-proposition
设$(X_1,X_2)^T\sim N(\mu,\Sigma)$，则$X_1,X_2$独立当且仅当$X_1,X_2$不相关
```

```ad-proof
因为$X_1,X_2$不相关，所以它们的相关矩阵$\Sigma$的非对角元素都为0。因此，它们的概率密度函数为：
$$
\begin{aligned}
f_{X_1,X_2}(x_1,x_2)&=\frac{1}{2\pi\sigma_1\sigma_2}\exp\left(-\frac{1}{2}\left(\frac{\left(x_1-\mu_1\right)^{2}}{\sigma_{1}^{2}}+\frac{\left(x_{2}-\mu_{2}\right)^{2}}{\sigma_{2}^{2}}\right)\right)\\
&=\frac{1}{\sqrt{2\pi}\sigma_1}\exp\left(-\frac{\left(x_1-\mu_{1}\right)^{2}}{2\sigma_{1}^{2}}\right)\frac{1}{\sqrt{2\pi}\sigma_2}\exp\left(-\frac{\left(x_{2}-\mu_{2}\right)^{2}}{2\sigma_{2}^{2}}\right)=f_{X_1}(x_1)f_{X_2}(x_2)
\end{aligned}
$$
所以$X_1,X_2$独立。

（n维情形也同理）
```

```ad-remark
我们之前学$K-L$分解的时候曾经学过去相关(PCA)，根据这个命题，我们可以用去相关操作使得联合高斯分布的各个随机变量之间相互独立。
```

## 高斯分布的条件分布仍然是高斯

```ad-proposition
设$X=(X_1,X_2)^T\in \mathbb R^{m+n},X\sim N$，则$X_2|X_1\sim N$
```

```ad-proof
我们先来看一下$X_2|X_1$的分布函数：
$$
f_{X_{2}|X_{1}}(x_{2}|x_1)=\frac{f_{X_{1},X_{2}}(x_{1},x_{2})}{f_{X_{1}}(x_{1})}=C\cdot \exp(...)
$$
其中$C$是常数，$\exp(...)$内是
$$
\left.-\frac{1}{2}\left(x_{1}^{T}-{\mu_{1}}^{T},x_{2}^{T}-{\mu_{2}}^{T}\right)\left(\begin{array}{cc}{{\sum_{11}}}&{{\sum_{12}}}\\{{\sum_{21}}}&{{\sum_{22}}}\\\end{array}\right.\right)^{-1}\left(\begin{array}{c}{{x_{1}-\mu_{1}}}\\{{x_{2}-\mu_{2}}}\\\end{array}\right)+\frac{1}{2}(x_{1}^{T}-\mu_{1}^{T})\Sigma_{11}^{-1}(x_{1}-\mu_{1})
$$
我们将矩阵$\left.\left(\begin{matrix}\sum_{11}&\sum_{12}\\\sum_{21}&\sum_{22}\end{matrix}\right.\right)$进行分块对角化得
$$
\left.\left(\begin{array}{cc}I&0\\\Sigma_{21}\Sigma_{11}^{-1}&I\end{array}\right.\right)\left(\begin{array}{cc}\sum_{11}&\sum_{12}\\\sum_{21}&\sum_{22}\end{array}\right)\left(\begin{array}{cc}I&-\sum_{11}^{-1}\Sigma_{12}\\0&I\end{array}\right)=\left(\begin{array}{cc}\Sigma_{11}&0\\0&\Sigma_{21}-\Sigma_{21}\sum_{11}^{-1}\Sigma_{12}\end{array}\right)
$$
即
$$
\left.\left(\begin{array}{cc}{{\Sigma_{11}}}&{{\Sigma_{12}}}\\{{\Sigma_{21}}}&{{\Sigma_{22}}}\\\end{array}\right.\right)^{-1}=\left(\begin{array}{cc}{I}&{{-{\Sigma}_{11}^{-1}{\Sigma}_{12}}}\\{0}&{I}\\\end{array}\right)\left(\begin{array}{cc}{{\Sigma_{11}^{-1}}}&{0}\\{0}&{{(\Sigma_{22}-{\Sigma}_{21}{\Sigma}_{11}^{-1}{\Sigma}_{12})^{-1}}}\\\end{array}\right)\left(\begin{array}{cc}{I}&{0}\\{{-\Sigma_{21}{\Sigma}_{11}^{-1}}}&{I}\\\end{array}\right)
$$
于是$\exp(...)$里面的系数就变成了：
$$
\begin{aligned}
&\left.-\frac{1}{2}\left(x_{1}^{T}-{\mu_{1}}^{T},x_{2}^{T}-{\mu_{2}}^{T}\right)\left(\begin{array}{cc}{{\sum_{11}}}&{{\sum_{12}}}\\{{\sum_{21}}}&{{\sum_{22}}}\\\end{array}\right.\right)^{-1}\left(\begin{array}{c}{{x_{1}-\mu_{1}}}\\{{x_{2}-\mu_{2}}}\\\end{array}\right)+\frac{1}{2}(x_{1}^{T}-\mu_{1}^{T})\Sigma_{11}^{-1}(x_{1}-\mu_{1})\\\\
&=\left.-\frac{1}{2}\left(x_{1}^{T}-\mu_{1}^{T},x_{2}^{T}-\mu_{2}^{T}\right)\left(\begin{array}{cc}{I}&{{-\Sigma_{11}^{-1}\Sigma_{12}}}\\{0}&{I}\\\end{array}\right.\right)\left(\begin{array}{cc}{\Sigma_{11}^{-1}}&{0}\\{0}&{{(\Sigma_{22}-\Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12})^{-1}}}\\\end{array}\right)\left(\begin{array}{cc}{I}&{0}\\{{-\Sigma_{21}\Sigma_{11}^{-1}}}&{I}\\\end{array}\right)\left(\begin{array}{c}{{x_{1}-\mu_{1}}}\\{{x_{2}-\mu_{2}}}\\\end{array}\right)\\&+\frac{1}{2}(x_{1}^{T}-\mu_{1}^{T})\Sigma_{11}^{-1}(x_{1}-\mu_{1})\\\\
&=-\frac{1}{2}(x_{1}^{T}-\mu_{1}^{T})\Sigma_{11}^{-1}(x_{1}-\mu_{1})-\frac{1}{2}\left(x_{2}^{T}-\mu_{2}^{T}-\left(x_{1}^{T}-\mu_{1}^{T}\right)\Sigma_{11}^{-1}\Sigma_{12}\right)\left(\Sigma_{22}-\Sigma_{21}\Sigma_{11}\Sigma_{12}\right)^{-1}\left(x_{2}-\mu_{2}-\Sigma_{21}\Sigma_{11}^{-1}\left(x_{1}-\mu_{1}\right)\right)\\
&+\frac{1}{2}(x_{1}^{T}-\mu_{1}^{T})\Sigma_{11}^{-1}(x_{1}-\mu_{1})\\\\
&=-\frac12\left(x_2^T-(\mu_2^T+(x_1^T-\mu_1^T)\Sigma_{11}^{-1}\Sigma_{12})\right)\left(\Sigma_{22}-\Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12}\right)^{-1}\left(x_2-(\mu_2+\Sigma_{21}\Sigma_{11}^{-1}(x_1-\mu_1))\right)
\end{aligned}
$$

我们很容易看出这是一个均值和协方差矩阵为如下的高斯分布：
$$
\begin{aligned}E(X_2\mid X_1)&=\mu_2+\Sigma_{21}\Sigma_{11}^{-1}(X_1-\mu_1)\\\\\Sigma_{X_2|X_1}&=\Sigma_{22}-\Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12}\end{aligned}
$$
```

```ad-info
一维的情形：
$$
\begin{aligned}E(X_{2}|X_{1})&=\mu_{2}+\frac{\sigma_{21}}{\sigma_{11}}(X_{1}-\mu_{1})\\\Sigma_{X_{2}|X_{1}}&=\sigma_{22}-\frac{(\sigma_{12})^{2}}{\sigma_{11}}\end{aligned}
$$
```

下面是上述结论的几个应用:
### (1)条件期望的几何含义：在均方意义下的最优逼近

设$X_1,X_2$是两个随机变量，我们希望找到一个函数g，使得$g(X_1)$在均方意义下最优逼近$X_2$，即$\underset{g}{\min}E|X_2-g(X_1)|^2$。事实上这个最优的函数g是$g_{opt}(X_1)=E(X_2|X_1)$。所以条件期望可以理解为在均方意义下的最优逼近。

下面来证明：

```ad-proof
$$
\begin{aligned}
E|X_{2}-g(X_{1})|^{2}&=E|X_{2}-E(X_{2}|X_{1})+E(X_{2}|X_{1})-g(X_{1})|^{2}\\
&=E\left(X_{2}-E(X_{2}|X_{1})\right)^{2}+E\left(E(X_{2}|X_{1})-g(X_{1})\right)^{2}+2E((X_{2}-E(X_{2}|X_{1}))(E(X_{2}|X_{1})-g(X_{1})))
\end{aligned}
$$
我们如果证明了$E((X_{2}-E(X_{2}|X_{1}))(E(X_{2}|X_{1})-g(X_{1})))=0$，我们的命题就成立了，下面我们来证明它。我们观察到，括号里面的$X_{2},E(X_{2}|X_{1}),E(X_{2}|X_{1}),g(X_{1})$都是随机变量（别忘了我们前面说的，条件期望是随机变量）。当有多个随机变量同时出现时，我们常常使用条件来做，于是有
$$
\begin{aligned}
&E((X_{2}-E(X_{2}|X_{1}))(E(X_{2}|X_{1})-g(X_{1})))\\
&=E_{X_1}\left(\left.E_{X_2}\left(\left.\left(X_{2}-E\left(X_{2}\right|X_{1}\right)\right)\left(\left.E\left(X_{2}|X_{1}\right)-g\left(X_{1}\right)\right)\right)\right|X_{1}\right)\right)
\end{aligned}
$$
其中
$$
\begin{aligned}
&E_{X_2}((X_2-E(X_2|X_1))(E(X_2|X_1)-g(X_1)))|X_1)\\
&=\left(E(X_2|X_1)-g(X_1)\right)E_{X_2}(X_2-E(X_2|X_1)|X_1)
\end{aligned}
$$
（上面的等式成立是因为在条件住$X_1$的情况下，$X_1$的随机性暂时消失了，所以$(E(X_2|X_1)-g(X_1))$是确定的，所以可以提到E外面去）

而上式中的$E_{X_2}(X_2-E(X_2|X_1)|X_1)=0$，这是因为：
$$
\begin{aligned}
&E_{X_{2}}(X_{2}-E(X_{2}|X_{1})|X_{1})\\
&=E(X_{2}|X_{1})-E(E(X_{2}|X_{1})|X_{1})\\
&=E(X_{2}|X_{1})-E(X_{2}|X_{1})E(1|X_{1})\\
&=0
\end{aligned}
$$
（上式第二个等式是因为在$X_1$的条件下，$E(X_2|X_1)$暂时是确定的，所以可以提出来）

所以这个命题就证完了。
```

### (2) 均方意义下的最优线性逼近证明(高维)

有的时候我们找不到最优逼近函数$g$，只能退而求其次，找$X_1$到$X_2$的最优线性逼近，即$\underset{a}{\min}E(X_2-aX_1)$。事实上，
$$
a_{opt}=\frac{E(X_1X_2)}{EX_1^2}
$$

下面开始证明,我们索性就直接证高维的版本。

设$Y\in \mathbb R^m,X\in \mathbb R^n$是高维随机变量，$A\in \mathbb R^{m\times n}$是常数矩阵。我们想求A使得
$$
\min_AE(Y-AX)^T\left(Y-AX\right)=\min_AE(Y^TY-Y^TAX-X^TA^TY+X^TA^TAX)
$$

我们有：
$$
\begin{aligned}(1)\nabla_A\operatorname{E}\left(Y^TAX\right)&=\operatorname{E}\left(\nabla_A\left(Y^TAX\right)\right)=\operatorname{E}\left(YX^T\right)\\\\(2)\nabla_A\operatorname{E}\left(X^TA^TY\right)&=\operatorname{E}\left(\nabla_A\left(Y^TAX\right)\right)=\operatorname{E}\left(YX^T\right)\\\\(3)\nabla_A\operatorname{E}\left(X^TA^TAX\right)&=\nabla_A\operatorname{E}\left(\operatorname{Tr}\left(X^TA^TAX\right)\right)\\\\&=\nabla_A\operatorname{E}\left(\operatorname{Tr}\left(AXX^TA^T\right)\right)\\\\&=\operatorname{E}\left(\nabla_A\left(\operatorname{Tr}(AXX^TA^T\right)\right)\big)\\\\&=2\operatorname{A}\operatorname{E}\left(XX^T\right)\end{aligned}
$$
```ad-info
我们用到了以下事实：
$$
\begin{aligned}&\nabla_A(y^TAx)=(y^T)^Tx^T\\&\nabla_A\mathrm{Tr}(ABA^T)=A(B+B^T)\end{aligned}
$$
其中(1)直接用就能证明；

(2)是因为$X^T$是躺下来的，$Y$是立起来的，所以$X^TA^TY$是一个标量，所以可以变成$Y^TAX$，然后再用上面的事实证明；

(3)是因为$X^TA^TAX$是标量，所以$X^TA^TAX=Tr(X^TA^TAX)$，然后转置之后不变，最后再用上面的事实就证出来了。
```
所以有：
$$
2E(XY^{T})=2AE(XX^{T})
$$
而$E(XY^{T})=R_{XY},E(XX^T)=R_{XX}$。所以有
$$
A=R_{XY}R_{XX}^{-1}
$$
当维度等于1时，就是上一点所讲的
$$
a_{opt}=\frac{E(X_1X_2)}{EX_1^2}
$$

### (3) 高斯分布的最优逼近和线性逼近是等价的

若$(X_1,X_2)\sim N$，我们根据前面(1)有
$$
E(X_{2}|X_{1})=\mu_{2}+\frac{\sigma_{21}}{\sigma_{11}}(X_{1}-\mu_{1})=\mu_{2}+\frac{E(X_1X_2)}{EX_1^2}(X_{1}-\mu_{1})
$$

即对于高斯分布，最优逼近和线性逼近是等价的（在均方意义下）。

## 高斯分布的性质的应用

下面我们利用前面得到的高斯分布的性质来证明一些问题：

### 高斯分布的样本均值和样本方差独立

```ad-definition
[样本均值和样本方差]

设$X_1,X_2,\cdots,X_n$是独立同分布的随机样本，称
$$
\overline{X}=\frac1n\sum_{k=1}^nX_k
$$
为样本均值；
$$
\overline{S}=\frac{1}{n-1}\sum_{k=1}^{n}(X_{k}-\overline{X})^{2}
$$
为样本方差。
```

我们很好奇为什么样本方差要除以n-1，而不是n？因为这能让样本方差是无偏的。下面是证明：

```ad-proof
$$
\begin{aligned}
E\left(\sum_{k=1}^n{(X_k-\bar{X})^2}\right)&=E\left(\sum_{k=1}^n{X_k}^2-2\left(\sum_{k=1}^n{X_k}\right)\bar{X}+n\left(\bar{X}\right)^2\right)\\&=E\left(\sum_{k=1}^{n}X_{k}^{2}-2\left(n\overline{X}\right)\overline{X}+n\left(\overline{X}\right)^{2}\right)\\
&=E\left(\sum_{k=1}^{n}X_{k}^{2}-n\left(\overline{X}\right)^{2}\right)
\end{aligned}
$$
而
$$
\begin{aligned}
E[(\overline{X})^{2}]&=\frac{1}{n^{2}}E[(\sum_{k=1}^{n}X_{k})^{2}]=\frac{1}{n^{2}}E(\sum_{k=1}^{n}X_{k}^{2}+\sum_{i\neq j}X_{i}X_{j})\\
&=\frac1{n^2}nE(X_1^2)+\frac{n(n-1)}{n^2}\mu^2\\\\&=\frac1nE(X_1^2)+\frac{n-1}n\mu^2
\end{aligned}
$$
所以
$$
\begin{aligned}&E\left(\sum_{k=1}^{n}X_k^2-n(\bar{X})^2\right)\\&=E\left(\sum_{k=1}^{n}X_k^2\right)-nE\left(\overline{X}\right)^2\\&=nE\left(X_1^2\right)-E\left(X_1^2\right)-\left(n-1\right)\mu^2\\&=(n-1)\left(E(X_1^2)-\mu^2\right)\\&=(n-1)\operatorname{var}\left(X_1\right)\end{aligned}
$$
即样本方差是无偏的。
```

下面我们来证明，当$X_i$服从的是高斯分布的时候，样本均值和样本方差是相互独立的。（这听起来很不可思议对吧，因为样本方差的公式中是含有样本均值的）

```ad-proposition
设$X_1,X_2,\cdots,X_n$是独立同服从高斯分布的随机样本，则$\overline{X}$与$\overline{S}$独立。
```

```ad-proof
由于
$$
\begin{cases}\overline X=\frac1n\sum_{k=1}^nX_k\\\\\overline{S}=\frac1{n-1}\sum_{k=1}^n\left(X_k-\overline{X}\right)^2=\frac1{n-1}\left(\sum_{k=1}^nX_k^2-n\left(\overline{X}\right)^2\right)&\end{cases}
$$
我们希望做一个线性变换：
$$
\begin{pmatrix}Y_1\\\vdots\\Y_n\end{pmatrix}=A\begin{pmatrix}X_1\\\vdots\\X_n\end{pmatrix}
$$
使得线性变换后的$Y_1,\cdots,Y_n$满足下面三个条件：

(1) $Y_i$间相互独立

(2) $\sum_{k=1}^nX_k^2=\sum_{k=1}^nY_k^2$

(3) $Y_1^2=n\left(\overline{X}\right)^2$

如果能找到线性变换A满足以上三点，则
$$
\begin{cases}{{\overline{X}=\frac{1}{\sqrt{n}}Y_{1}}}\\\\{{\overline{S}=\frac{1}{n-1}(\sum_{k=2}^{n}Y_{k}^{2})}}\end{cases}
$$
由于$Y_i$之间相互独立，于是就能证得$\overline{X}$与$\overline{S}$独立。

下面我们来构造A使其满足上面三个条件。

首先，根据我们前面关于多元高斯分布的知识，$Y_1,\cdots,Y_n$的联合分布的协方差矩阵为
$$
\Sigma_{(Y_1,\cdots,Y_n)}=A\Sigma_{(X_1,\cdots,X_n)}A^T
$$
而由于$X_1,\cdots,X_n$独立同分布，所以
$$
\Sigma_{(X_{1},\cdots,X_{n})}=\sigma^{2}I
$$
我们想要满足第一个条件，即想要$\Sigma_{(Y_1,\cdots,Y_n)}$为对角阵，即
$$
\Sigma_{(Y_1,\cdots,Y_n)}=A\Sigma_{(X_1,\cdots;X_n)}A^T=A\sigma
{ #2IA}
^T=diag
$$
此时我们只需要令A为正交矩阵($AA^T=I$)即可。而如果A是正交矩阵，那么第二个条件就自然满足了，因为：
$$
Y^{T}Y=X^{T}A^{T}AX=X^{T}X
$$
为了满足第三个条件，我们构造A的第一排为
$$
\left.\left(\begin{array}{c}Y_1\\\vdots\\Y_n\end{array}\right.\right)=\left(\begin{array}{ccc}\frac1{\sqrt{n}}&\cdots&\frac1{\sqrt{n}}\\\\\\\\\end{array}\right)\left(\begin{array}{c}X_1\\\vdots\\X_n\end{array}\right)
$$
此时就满足第三个条件了，因为：
$$
\begin{aligned}Y_{1}&=\frac{1}{\sqrt{n}}\sum_{k=1}^{n}X_{k}=\sqrt{n}\overline{X}\\Y_{1}{}^{2}&=n(\overline{X})^{2}\end{aligned}
$$
最后我们将$(\sqrt{n},\cdots,\sqrt{n})$扩展为n维空间的一组基，然后对这组基进行施密特正交化即可得到正交矩阵A。
```

```ad-info
在上面的证明中，我们看到
$$
{\overline{S}=\frac{1}{n-1}(\sum_{k=2}^{n}Y_{k}^{2})}
$$
这说明样本方差只有n-1个自由度，所以怪不得样本方差的定义中要除以n-1，而不是n了。
```

# 高斯过程

```ad-definition
[高斯过程]

设$X(t)$是一个随机过程，若对$\forall n,\forall t_1,t_2,\cdots,t_n$，n维随机向量$(X(t_1),\cdots,X(t_n))^T=X$服从n维高斯分布，即$X\sim N(\mu,\Sigma)$，则称$X(t)$是高斯过程。
```

# 高斯过程通过非线性系统

设$X(t)$是一个高斯过程，我们想探究它经过一个非线性系统$Y(t)=g(X(t))$后会发生什么变化。而且由于我们现在说到了相关,所以我们还尝试算出它们的相关函数.

## 通过二次系统

设$X(t)$是高斯过程，$Y(t)=X^2(t)$。假设$X(t)$是宽平稳，则$Y(t)$的均值为
$$
E(Y(t))=E(X(t)X(t))=R_X(t,t)=R_X(0)
$$

$Y(t)$的相关函数为
$$
R_{Y}(t,s)=E\left(Y(t)Y(s)\right)=E\left(X^{2}(t)X^{2}(s)\right)
$$

```ad-info
利用特征函数来求矩的方法：

设$X=(X_1,\cdots,X_n)$是n维随机变量，则其特征函数为$\phi_X(w)=E(\exp(jw_1X_1+\cdots+w_nX_n)))$。于是
$$
\begin{aligned}E(X_1)&=\frac1j\frac\partial{\partial w_1}\phi_X(w_1,\cdots,w_n)\bigg|_{w_1=\cdots=w_n=0}\\\\E(X_k^\alpha)&=\frac1{j^\alpha}\frac{\partial^\alpha}{\partial w_k^\alpha}\phi_X(w_1,\cdots,w_n)\bigg|_{w_1=\cdots=w_n=0}\\\\
E\left(X_1^{\alpha_1}\cdots X_n^{\alpha_n}\right)&=\frac1{j^{\alpha_1+\cdots+\alpha_n}}\quad\frac{\partial^{\alpha_1+\cdots+\alpha_n}}{\partial{w_1}^{\alpha_1}\cdots\partial{w_n}^{\alpha_n}}\phi_X\left(w_1,\cdots,w_n\right)\bigg|_{w_1=\cdots=w_n=0}.\end{aligned}
$$
求矩是积分运算，但特征函数将其变成了微分运算，这样算起来更容易。（微分运算就像拆表，积分运算就像装表。拆表比较简单而装表比较难）
```

```ad-info
利用上述特征函数法来求联合高斯分布的矩：

设$(X_1,X_2,X_3,X_4)$服从联合高斯，$E(X_k)=0$，求$E(X_1X_2X_3X_4)$。

根据高斯分布的特征函数：
$$
\left.\left.\phi_X(w)=\exp\left(-\frac12\left(w_1,w_2,w_3,w_4\right)\left(\operatorname{E}(x_ix_j)\right)_{ij}\left(\begin{array}{c}w_1\\w_2\\w_3\\w_4\end{array}\right.\right.\right)\right)
$$
利用上述方法最后算得
$$
E(X_{1}X_{2}X_{3}X_{4})=E(X_{1}X_{2})E(X_{3}X_{4})+E(X_{1}X_{3})E(X_{2}X_{4})+E(X_{1}X_{4})E(X_{2}X_{3})
$$
```

于是
$$
\begin{aligned}E\left(X^2(t)X^2(s)\right)&=E\left(X^2(t)\right)E\left(X^2(s)\right)+2\left(E\left(X(t)X(s)\right)\right)^2\\&=R_X^2(0)+2R_X^2(t-s)\end{aligned}
$$

## 通过符号函数系统

设$X(t)$是高斯过程，$Y(t)=g\left(X(t)\right)=sgn(X(t))=\begin{cases}1,X(t)\geq0\\-1,X(t)<0\end{cases}$，假设$X(t)$宽平稳，$E(X(t))=0$。

我们首先算$Y(t)$的均值为
$$
E(Y(t))=1\cdot P(X(t)\geq0)+(-1)P(X(t)<0)=0
$$

我们计算$Y(t)$的相关函数为：
$$
\begin{aligned}
R_Y(t,s)&=E\left(Y(t)Y(s)\right)=1\cdot P\left(Y(t)Y(s)=1\right)+(-1)\cdot P\left(Y(t)Y(s)=-1\right)\\
&=1\cdot P(Y(t)Y(s)=1)+(-1)(1-P(Y(t)Y(s)=1))\\
&=2\cdot P(Y(t)Y(s)=1)-1\\&=2P(X(t)X(s)\geq0)-1
\end{aligned}
$$

下面我们来计算$P(X(t)X(s)\geq0)$：
$$
P(X_1X_2\geq0)=(\int_{-\infty}^{0}\int_{-\infty}^{0}+\int_{0}^{\infty}\int_{0}^{\infty})\frac1{2\pi\sigma_1\sigma_2\sqrt{1-\rho^2}}\exp\left(-\frac1{2(1-\rho^2)}\left(\left(\frac{x_1}{\sigma_1}\right)^2+\left(\frac{x_2}{\sigma_2}\right)^2-2\rho\left(\frac{x_1}{\sigma_1}\right)\left(\frac{x_2}{\sigma_2}\right)\right)\right)dx_1dx_2
$$

算这个要经历四次换元：

(1) 我们令$x_{1}^{\prime}=\frac{x_{1}}{\sigma_{1}},x_{2}^{\prime}=\frac{x_{2}}{\sigma_{2}}$，则根据对称性有
$$
=\int_{0}^{\infty}\int_{0}^{\infty}\frac{2}{2\pi\sqrt{1-\rho^{2}}}\exp\left(-\frac{1}{2(1-\rho^{2})}\left(\left(x_{1}^{\prime}\right)^{2}+\left(x_{2}^{\prime}\right)^{2}-2\rho(x_{1}^{\prime})\left(x_{2}^{\prime}\right)\right)\right)dx_{1}^{\prime}dx_{2}^{\prime}
$$

(2) 我们令$\begin{cases}x_{1}^{\prime}=u+v\\x_{2}^{\prime}=u-v\end{cases}$，则变成了：

$$
=\underset{\text{积分区域1}}{\int\int}\frac{4}{2\pi\sqrt{1-\rho^2}}\exp\left(-\frac{1}{2\left(1-\rho^{2}\right)}\left(2\left(u^{2}+v^{2}\right)-2\rho\left(u^{2}-v^{2}\right)\right)\right)dudv 
$$

其中积分区域1为：
![](/img/user/课堂笔记/随机过程/attachments/高斯过程-1.png)

(3) 我们再令$\left.\left\{\begin{array}{c}{{u^{\prime}=\frac{u}{\sqrt{1+p}}}}\\{{v^{\prime}=\frac{v}{\sqrt{1-p}}}}\\\end{array}\right.\right.$，则有
$$
=\underset{\text{积分区域2}}{\int\int}\frac{2}{\pi}\exp\left(-(u^{\prime})^{2}-(v^{\prime})^{2}\right)du^{\prime}dv^{\prime}
$$

其中积分区域2为：
![](/img/user/课堂笔记/随机过程/attachments/高斯过程-2.png)

(4) 最后令$\begin{cases}u^{\prime}=r\cos\theta\\v^{\prime}=r\sin\theta\end{cases}$，则有
$$
\begin{aligned}&=\int_{-\phi}^{\phi}\int_{0}^{\infty}\frac2\pi\exp{(-r^2)}\mathrm{~r~d}r\mathrm{d}\theta\\&=\frac2{\pi}\phi\end{aligned}
$$

其中
$$
\phi=\arctan\left(\frac1{\sqrt{1-p}}\bigg/\frac1{\sqrt{1+p}}\right)=\arctan\left(\sqrt{\frac{1+p}{1-p}}\right)
$$

```ad-info
万能公式：设$t=\tan(\theta)$，则
$$
\cos(2\theta)=\frac{1-t^{2}}{1+t^{2}},\sin(2\theta)=\frac{2t}{1+t^{2}}
$$
```

根据万能公式和$\arcsin(x)+\arccos(x)=\frac{\pi}{2}$，我们可得
$$
\begin{aligned}
2\phi=\arccos\left(\frac{1-\frac{1+\rho}{1-\rho}}{1+\frac{1+\rho}{1-\rho}}\right)=\arccos\left(-\rho\right)&=\frac{\pi}2-\arcsin\left(-\rho\right)\\
=\frac{\pi}{2}+\arcsin(\rho)
\end{aligned}
$$
综上，我们可得
$$
\begin{aligned}P\left(X(t)X(s)\geqslant0\right)&=\frac1{\pi}\left(\frac\pi2+\arcsin(\rho)\right)\\\\&=\frac12+\frac1\pi\arcsin(\rho)\end{aligned}
$$
代回去得
$$
R_{Y}(t,s)=\frac2{\pi}\arcsin(\rho)
$$

```ad-remark
这个结果说明$R_Y$只跟$\rho$有关，而$\rho=\frac{E(X(t)X(s))}{(E(X^{2}(t))E(X^{2}(s)))^{\frac{1}{2}}}=\frac{R_{X}(t-s)}{R_{X}(0)}$，这说明$Y(t)$是宽平稳的。
```

## 通过非线性系统的一般方法

假设$g(x_1,x_2)$为非线性函数，$(X_1,X_2)\sim N(0,0,\sigma_1^2,\sigma_2^2,\rho)$，我们想要计算$E(g(X_1,X_2))$。有没有一般的方法呢？

```ad-proposition
[Price Theorem]

设$g(x_1,x_2)$为非线性函数，$(X_1,X_2)\sim N(0,0,\sigma_1^2,\sigma_2^2,\rho)$，则
$$
\frac{\partial^{2}E(g(x_{1},x_{2}))}{\partial \rho}=\sigma_{1}\sigma_{2}~E\left(\frac{\partial^{2}g(x_{1},x_{2})}{\partial x_{1}\partial x_{2}}\right)
$$
其中，$g(x_1,x_2)$还要满足$g(x_1,x_2)f(x_1,x_2)\to 0(x_1\to \infty,x_2\to \infty)$，其中$f(x_1,x_2)$为二维高斯分布的概率密度函数

（但这个条件往往能够得到满足，因为这个条件就是要求当$x_1,x_2$趋向于无穷大的时候，$g(x_1,x_2)$趋向于无穷大的速度比指数上方二次要慢（因为高斯分布的概率密度函数是指数上方二次的））
```

```ad-info
我们运用这个命题一般有以下四部曲：

(1) 确定$g(x_1,x_2)$

(2) 求二阶导$\frac{\partial^2g(x_1,x_2)}{\partial x_1\partial x_2}$

(3) 计算期望$E\left(\frac{\partial^2g(x_1,x_2)}{\partial x_1\partial x_2}\right)$

(4) 根据$\frac{\partial E(g(x_1,x_2))}{\partial\rho}=\sigma_1\sigma_2E\left(\frac{\partial^2g(x_1,x_2)}{\partial x_1\partial x_2}\right)$得到$\frac{\partial E(g(x_1,x_2))}{\partial\rho}$，然后对其积分得到$E(g(x_1,x_2))$

```

我们先用前面的问题来来试试这个命题的威力：

```ad-info
对于前面的第二个问题：设$X(t)$是高斯过程，$Y(t)=g\left(X(t)\right)=sgn(X(t))=\begin{cases}1,X(t)\geq0\\-1,X(t)<0\end{cases}$，假设$X(t)$宽平稳。我们想计算$Y(t)$的相关函数$R_Y(t,s)=E\left(Y(t)Y(s)\right)$：

(1) 设$g(x_{1},x_{2})=sgn(x_{1})sgn(x_{2})$，要求$R_{Y}(t,s)=E\left(Y(t)Y(s)\right)=E\left(sgn(X(t))\cdot sgn(X(s))\right)$

(2) $\frac{\partial^2g(x_1,x_2)}{\partial x_1\partial x_2}=4\delta(x_1)\delta(x_2)$

(3) 由于$X_1,X_2$联合高斯，所以
$$
\begin{aligned}
&E\left(\frac{\partial^2g(x_1,x_2)}{\partial x_1\partial x_2}\right)\\\\
&=\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}\frac{4\delta(x_{1})\delta(x_{2})}{2\pi\sigma_{1}\sigma_{2}\sqrt{1-\rho^{2}}}\exp\left(-\frac{1}{2(1-\rho^{2})}\left(\left(\frac{x_{1}}{\sigma_{1}}\right)^{2}+\left(\frac{x_{2}}{\sigma_{2}}\right)^{2}-2\rho\left(\frac{x_{1}}{\sigma_{1}}\right)\left(\frac{x_{2}}{\sigma_{2}}\right)\right)\right)dx_{1}dx_{2}\\\\
&=\frac{2}{\pi}\frac{1}{\sigma_{1}\sigma_{2}}\frac{1}{\sqrt{1-\rho^{2}}}
\end{aligned}
$$
(4) 
$$
\begin{aligned}\frac{\partial E(g(x_1,x_2))}{\partial\rho}&=\sigma_1\sigma_2E\left(\frac{\partial^2g\left(x_1,x_2\right)}{\partial x_1\partial x_2}\right)\\&=\frac2{\pi}\frac1{\sqrt{1-\rho^2}}\end{aligned}
$$
于是
$$
\begin{aligned}
E(g(x_{1},x_{2}))&=\frac{2}{\pi}\int_{0}^{\rho}\frac{1}{\sqrt{1-t^{2}}}dt+E(g(x_{1},x_{2}))\bigg|_{\rho=0}\\
&=\frac{2}{\pi}\arcsin(\rho)
\end{aligned}
$$

```

(卧槽!这么容易就做出来了!卧槽!NB!)

```ad-info
对于前面的第一个问题：设$X(t)$是高斯过程，$Y(t)=X^2(t)$。假设$X(t)$是宽平稳，我们想求$Y(t)$的相关函数$R_{Y}(t,s)=E\left(Y(t)Y(s)\right)$：

(1) $g(x_1,x_2)=x_1^2x_2^2$

(2) $\frac{\partial^{2}g}{\partial x_{1}\partial x_{2}}=4x_{1}x_{2}$

(3) $E(\frac{\partial^{2}g}{\partial x_{1}\partial x_{2}})=4E(X(t)X(s))=4R_{X}(t-s)$

(4) 因为$\begin{cases}{{\sigma_{1}}^{2}=E({X_{1}}^{2})=E({X^2(t)})=R_{X}(0)}\\{{\sigma_{2}}^{2}=R_{X}(0)}&\end{cases}$，所以
$$
\begin{aligned}
\frac{\partial}{\partial\rho}E\left(g\left(X_{1},X_{2}\right)\right)&=4\sigma_1\sigma_{2}R_X\left(t-s\right)\\
&=4R_{X}(0)R_{X}(t-s)\\
&=4R^{2}_X(0)\cdot\rho 
\end{aligned}
$$
所以
$$
\begin{aligned}
E(g(X_{1},X_{2}))&=E(g(X_{1},X_{2}))\bigg|_{\rho=0}+\int_{0}^{\rho}R_{X}^{2}(0)\cdot4tdt\\
&=R_{X}^{2}(0)+R_{X}^{2}(0)\cdot2\rho^{2}\\&=R_{X}^{2}(0)+2R_{X}^{2}(t-s)
\end{aligned}
$$
```

看到了吧，之前我们累死累活做的结果，用这个命题很轻松就做出来了。下面我们来证明这个命题：

```ad-proposition
[Price Theorem]

设$g(x_1,x_2)$为非线性函数，$(X_1,X_2)\sim N(0,0,\sigma_1^2,\sigma_2^2,\rho)$，则
$$
\frac{\partial^{2}E(g(x_{1},x_{2}))}{\partial \rho}=\sigma_{1}\sigma_{2}~E\left(\frac{\partial^{2}g(x_{1},x_{2})}{\partial x_{1}\partial x_{2}}\right)
$$
其中，$g(x_1,x_2)$还要满足$g(x_1,x_2)f(x_1,x_2)\to 0(x_1\to \infty,x_2\to \infty)$，其中$f(x_1,x_2)$为二维高斯分布的概率密度函数

（但这个条件往往能够得到满足，因为这个条件就是要求当$x_1,x_2$趋向于无穷大的时候，$g(x_1,x_2)$趋向于无穷大的速度比指数上方二次要慢（因为高斯分布的概率密度函数是指数上方二次的））
```

```ad-proof
设$f(x_1,x_2)$是二维高斯分布的概率密度函数，即
$$
f(x_1,x_2)=\frac{1}{2\pi \sigma_{1}\sigma_{2}\sqrt{1-\rho^{2}}}\exp\left(-\frac{1}{2(1-\rho^{2})}\left(\left(\frac{x_{1}}{\sigma_{1}}\right)^{2}+\left(\frac{x_{2}}{\sigma_{2}}\right)^{2}-2\rho\left(\frac{x_{1}}{\sigma_{1}}\right)\left(\frac{x_{2}}{\sigma_{2}}\right)\right)\right)
$$
于是
$$
E\left(g(x_{1},x_{2})\right)=\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}g(x_{1},x_{2})f(x_{1},x_{2})dx_{1}dx_{2}
$$
因为$X_1,X_2$是联合高斯的，所以它们的特征函数为
$$
\begin{aligned}
\phi(w_1,w_2)&=\exp(-\frac12(w_1,w_2)R\left.\left(\begin{matrix}w_{1}\\w_{2}\end{matrix}\right.\right))\\
&=\exp\left(-\frac{1}{2}\left(\sigma_{1}{}^{2}w_{1}{}^{2}+\sigma_{2}{}^{2}w_{2}{}^{2}+2\rho\sigma_{1}\sigma_{2}w_{1}w_{2}\right)\right)
\end{aligned}
$$
其中
$$
\left.R=\left(\begin{array}{ccc}{{E({X_{1}}^{2})}}&{{{E}\left(X_{1}X_{2}\right)}}\\{{E(X_{1}X_{2})}}&{{E(X_{2}^{2})}}\\\end{array}\right.\right)=\left(\begin{array}{cc}{{\sigma_{1}}^{2}}&{{\rho\sigma_{1}\sigma_{2}}}\\{{\rho\sigma_{1}{\sigma_{2}}}}&{{\sigma_{2}}^{2}}\\\end{array}\right)
$$
由于$\phi(w_1,w_2)$是$f(x_1,x_2)$的傅里叶变换，所以有$f(x_1,x_2)$是$\phi(w_1,w_2)$的傅里叶逆变换，即
$$
f(x_{1},x_{2})=\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}\phi(w_{1},w_{2})\exp\left(-j(w_{1}x_{1}+w_{2}x_{2})\right)dw_1dw_{2}
$$
于是有
$$
\begin{aligned}&\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}g\left(x_{1},x_{2}\right)f(x_{1},x_{2})dx_{1}dx_{2}\\&=\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}g\left(x_{1},x_{2}\right)\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}\phi\left(w_{1},w_{2}\right)\exp\left(-j\left(w_{1}x_{1}+w_{2}x_{2}\right)\right)dw_1dw_{2}dx_1dx_2\end{aligned}
$$
上式只有$\phi(w_1,w_2)$与$\rho$有关，所以有
$$
\begin{aligned}
\frac{\partial}{\partial \rho}E(g(x_{1},x_{2}))&=\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}g(x_{1},x_{2})\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}\left(-\sigma_{1}\sigma_{2}w_{1}w_{2}\right)\phi(w_{1},w_{2})\exp\left(-j(w_{1}x_{1}+w_{2}x_{2})\right)dw_{1}dw_{2}dx_1dx_{2}\\\\
&=\sigma_{1}\sigma_{2}\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}g(x_{1},x_{2})\frac{\partial^{2}}{\partial x_{1}\partial x_{2}}\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}\phi(w_{1},w_{2})\exp(-j(w_{1}x_{1}+w_{2}x_{2}))dw_1dw_{2}dx_1dx_{2}\\\\
&=\sigma_{1}\sigma_{2}\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}g(x_{1},x_{2})\frac{\partial^{2}}{\partial x_{1}\partial x_{2}}f(x_{1},x_{2})dx_{1}dx_{2}\\\\
&=\sigma_{1}\sigma_{2}\int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty}\left(\frac{\partial^{2}}{\partial x_{1}\partial x_{2}}g(x_{1},x_{2})\right)f(x_{1},x_{2})dx_{1}dx_{2}\\\\
&=\sigma_{1}\sigma_{2}E\left(\frac{\partial^{2}g(x_{1},x_{2})}{\partial x_{1}\partial x_{2}}\right)
\end{aligned}
$$
（倒数第二个等式是用了两次分部积分公式，因为要求$g(x_1,x_2)f(x_1,x_2)\to 0$，所以别的项都没了；而且分部积分了两次，所以分部积分产生的负号也没了）
```
